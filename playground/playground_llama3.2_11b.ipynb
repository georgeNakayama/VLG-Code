{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w4756677/miniconda3/envs/llama3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/home/w4756677/garment/AIpparel-Code/src')\n",
    "from transformers import AutoProcessor, LlavaConfig\n",
    "import os \n",
    "\n",
    "from data.datasets.gcd_dataset import GarmentCodeData\n",
    "from data.datasets.panel_configs import StandardizeConfig, StatsConfig\n",
    "from data.garment_tokenizers.garment_tokenizer_for_regression import GarmentTokenizerForRegression\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PanelClasses::INFO::Loading panel classes from file:  /home/w4756677/garment/AIpparel-Code/assets/panel_classes_garmentcodedata.json\n",
      "The panel classes in this dataset are : ['left_btorso', 'left_ftorso', 'right_ftorso', 'right_btorso', 'skirt_front', 'skirt_back', 'wb_back', 'wb_front', 'left_sleeve_b', 'left_sleeve_f', 'right_sleeve_f', 'right_sleeve_b', 'sl_left_cuff_skirt_f', 'sl_left_cuff_skirt_b', 'sl_right_cuff_skirt_b', 'sl_right_cuff_skirt_f', 'sl_left_cuff_b', 'sl_left_cuff_f', 'sl_right_cuff_f', 'sl_right_cuff_b', 'pant_f_l', 'pant_b_l', 'pant_f_r', 'pant_b_r', 'left_collar_front', 'left_collar_back', 'right_collar_front', 'right_collar_back', 'ins_skirt_front_0', 'ins_skirt_back_0', 'ins_skirt_back_1', 'ins_skirt_front_1', 'skirt_panel_2', 'skirt_panel_0', 'skirt_panel_1', 'skirt_panel_3', 'skirt_panel_4', 'skirt_panel_5', 'ins_skirt_back_2', 'ins_skirt_front_2', 'skirt_panel_6', 'skirt_front_0', 'skirt_back_0', 'left_hood', 'right_hood', 'skirt_panel_7', 'skirt_panel_8', 'pant_l_cuff_skirt_f', 'pant_l_cuff_skirt_b', 'pant_r_cuff_skirt_b', 'pant_r_cuff_skirt_f', 'ins_skirt_front_3', 'ins_skirt_back_3', 'pant_l_cuff_f', 'pant_l_cuff_b', 'pant_r_cuff_f', 'pant_r_cuff_b', 'skirt_back_1', 'skirt_front_1', 'skirt_panel_9', 'skirt_panel_10', 'ins_skirt_back_4', 'ins_skirt_front_4', 'skirt_front_2', 'skirt_back_2', 'skirt_panel_11', 'skirt_panel_12', 'skirt_back_3', 'skirt_front_3', 'skirt_panel_13', 'ins_skirt_front_5', 'ins_skirt_back_5', 'skirt_back_4', 'skirt_front_4', 'skirt_panel_14']\n",
      "PanelClasses::INFO::Loading panel classes from file:  /home/w4756677/garment/AIpparel-Code/assets/panel_classes_garmentcodedata.json\n",
      "The panel classes in this dataset are : ['left_btorso', 'left_ftorso', 'right_ftorso', 'right_btorso', 'skirt_front', 'skirt_back', 'wb_back', 'wb_front', 'left_sleeve_b', 'left_sleeve_f', 'right_sleeve_f', 'right_sleeve_b', 'sl_left_cuff_skirt_f', 'sl_left_cuff_skirt_b', 'sl_right_cuff_skirt_b', 'sl_right_cuff_skirt_f', 'sl_left_cuff_b', 'sl_left_cuff_f', 'sl_right_cuff_f', 'sl_right_cuff_b', 'pant_f_l', 'pant_b_l', 'pant_f_r', 'pant_b_r', 'left_collar_front', 'left_collar_back', 'right_collar_front', 'right_collar_back', 'ins_skirt_front_0', 'ins_skirt_back_0', 'ins_skirt_back_1', 'ins_skirt_front_1', 'skirt_panel_2', 'skirt_panel_0', 'skirt_panel_1', 'skirt_panel_3', 'skirt_panel_4', 'skirt_panel_5', 'ins_skirt_back_2', 'ins_skirt_front_2', 'skirt_panel_6', 'skirt_front_0', 'skirt_back_0', 'left_hood', 'right_hood', 'skirt_panel_7', 'skirt_panel_8', 'pant_l_cuff_skirt_f', 'pant_l_cuff_skirt_b', 'pant_r_cuff_skirt_b', 'pant_r_cuff_skirt_f', 'ins_skirt_front_3', 'ins_skirt_back_3', 'pant_l_cuff_f', 'pant_l_cuff_b', 'pant_r_cuff_f', 'pant_r_cuff_b', 'skirt_back_1', 'skirt_front_1', 'skirt_panel_9', 'skirt_panel_10', 'ins_skirt_back_4', 'ins_skirt_front_4', 'skirt_front_2', 'skirt_back_2', 'skirt_panel_11', 'skirt_panel_12', 'skirt_back_3', 'skirt_front_3', 'skirt_panel_13', 'ins_skirt_front_5', 'ins_skirt_back_5', 'skirt_back_4', 'skirt_front_4', 'skirt_panel_14']\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GarmentCodeData(\n",
    "    root_dir=\"/miele/timur/garmentcodedata\",\n",
    "    editing_dir=\"/mnt/gs/sci-garment/garmentcodedata_editing\",\n",
    "    caption_dir=\"/mnt/gs/sci-garment/long-caption-processed\",\n",
    "    editing_flip_prob=0.5,\n",
    "    sampling_rate=[0, 0, 0, 0, 1],\n",
    "    split_file=\"/home/w4756677/garment/AIpparel-Code/assets/garmentcodedata_datasplit.json\",\n",
    "    datalist_file=\"/home/w4756677/garment/AIpparel-Code/assets/garmentcodedata_list.txt\",\n",
    "    panel_classification=\"/home/w4756677/garment/AIpparel-Code/assets/panel_classes_garmentcodedata.json\",\n",
    "    body_type=\"default_body\",\n",
    "    split=\"train\"\n",
    ")\n",
    "val_dataset = GarmentCodeData(\n",
    "    root_dir=\"/miele/timur/garmentcodedata\",\n",
    "    editing_dir=\"/mnt/gs/sci-garment/garmentcodedata_editing\",\n",
    "    caption_dir=\"/mnt/gs/sci-garment/long-caption-processed\",\n",
    "    editing_flip_prob=0.5,\n",
    "    sampling_rate=[0.5, 0.5,0,0,0],\n",
    "    split_file=\"/home/w4756677/garment/AIpparel-Code/assets/garmentcodedata_datasplit.json\",\n",
    "    datalist_file=\"/home/w4756677/garment/AIpparel-Code/assets/garmentcodedata_list.txt\",\n",
    "    panel_classification=\"/home/w4756677/garment/AIpparel-Code/assets/panel_classes_garmentcodedata.json\",\n",
    "    body_type=\"default_body\",\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "gt_stats = StandardizeConfig(\n",
    "    rotations=StatsConfig(shift=[0, 0, 0, 0], scale=[1, 1, 1, 1]),\n",
    "    translations=StatsConfig(shift=[-1.25378371e-02,  1.13507532e+02,  2.63046369e+00], scale=[26.06867645, 32.42920198, 22.29905009]),\n",
    "    vertices=StatsConfig(shift=[8.44428116, 16.84081321], scale=[24.4920733,  26.60402835])\n",
    ")\n",
    "\n",
    "garment_tokenizer = GarmentTokenizerForRegression(\n",
    "    standardize=gt_stats,\n",
    "    random_tag=True,\n",
    "    num_tags=108,\n",
    "    include_template_name=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "# processor.tokenizer.add_tokens(\"<pad>\", special_tokens=True)\n",
    "# processor.tokenizer.pad_token = \"<pad>\"\n",
    "processor.tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "all_new_tokens = garment_tokenizer.get_all_token_names()\n",
    "num_added_tokens = processor.tokenizer.add_tokens(all_new_tokens, special_tokens=True)\n",
    "token_name2_idx_dict = {}\n",
    "for token in all_new_tokens:\n",
    "    token_idx = processor.tokenizer(token, add_special_tokens=False).input_ids[0]\n",
    "    token_name2_idx_dict[token] = token_idx\n",
    "    \n",
    "garment_tokenizer.set_token_indices(token_name2_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128379\n"
     ]
    }
   ],
   "source": [
    "from data.collate_fns import collate_fn\n",
    "print(len(processor.tokenizer))\n",
    "data = train_dataset[0]\n",
    "dialog = data[2]\n",
    "chat = processor.apply_chat_template(dialog, tokenize=False)\n",
    "data_dict = collate_fn(\n",
    "    [train_dataset[0], train_dataset[1], train_dataset[2], train_dataset[3]], \n",
    "    processor=processor, \n",
    "    garment_tokenizer=garment_tokenizer,\n",
    "    model_version=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 279]) torch.Size([1, 279])\n"
     ]
    }
   ],
   "source": [
    "print(data_dict[\"input_ids\"].shape, data_dict[\"labels\"].shape)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    data_dict = collate_fn(\n",
    "    [train_dataset[i]], \n",
    "    processor=processor, \n",
    "    garment_tokenizer=garment_tokenizer,\n",
    "    model_version=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "    if 128265 in data_dict[\"pattern_params\"].keys():\n",
    "        if data_dict[\"pattern_params\"][128265].shape[-1] != 2:\n",
    "            print(i, data_dict[\"pattern_params\"][128265].shape)\n",
    "# print(processor.tokenizer.decode([128256, 128265]))\n",
    "# print(data_dict[\"labels\"].max(), data_dict[\"labels\"][data_dict[\"labels\"] != -100].min())\n",
    "# for k in data_dict[\"pattern_params\"].keys():\n",
    "#     print(k)\n",
    "#     print(data_dict[\"pattern_params\"][k][data_dict[\"pattern_params_mask\"][k]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "sample_dict = {}\n",
    "sample_dict[\"user\"] = \"Question: What do respiration and combustion give out\\nChoices:\\nA. Oxygen\\nB. Carbon dioxide\\nC. Nitrogen\\nD. Heat\\nAnswer with the letter.\"\n",
    "sample_dict[\"assistant\"] = \"Answer: B\"\n",
    "dialog = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\": \"image\"}, {\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}]},\n",
    "    {\"role\":\"assistant\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}]}\n",
    "]\n",
    "sample_dict[\"user\"] = \"Question: In the given food web, which are the organism that only eaten roadrunner?\\nChoices:\\nA. dingo, jack rabbit\\nB. coyote, bobcat\\nC. dingo, bobcat\\nD. snake, jack rabbit\\nAnswer with the letter.\"\n",
    "sample_dict[\"assistant\"] = \"Answer: B\"\n",
    "dialog += [\n",
    "    {\"role\":\"user\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}]},\n",
    "    {\"role\":\"assistant\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>>\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <image>\n",
      "Question: What do respiration and combustion give out\n",
      "Choices:\n",
      "A. Oxygen\n",
      "B. Carbon dioxide\n",
      "C. Nitrogen\n",
      "D. Heat\n",
      "Answer with the letter. [/INST] Answer: B<\\s> [INST] Question: In the given food web, which are the organism that only eaten roadrunner?\n",
      "Choices:\n",
      "A. dingo, jack rabbit\n",
      "B. coyote, bobcat\n",
      "C. dingo, bobcat\n",
      "D. snake, jack rabbit\n",
      "Answer with the letter. [/INST] Answer: B<\\s> \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,LlavaNextProcessor\n",
    "# dialog = [\n",
    "#     {\"role\":\"user\",\"content\":[\n",
    "#         {\"type\": \"image\"}, \n",
    "#         {\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}\n",
    "#         ]},\n",
    "#     {\"role\":\"assistant\",\"content\":[\n",
    "#         {\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}\n",
    "#         ]}\n",
    "# ]\n",
    "# dialog = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "#         ],\n",
    "#     },\n",
    "# ]\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "text_prompt = processor.apply_chat_template(dialog, tokenize=False, add_generation_prompt=False)\n",
    "print(text_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1  2087 18741  4060    13 28741 10706  1444   264 13903  2188   304\n",
      "    396 18278 10895 13892 28723   415 13892  5212 10865 28725 10537 28725\n",
      "    304 27057 11194   298   272  2188 28742 28713  4224 28723    13 28789\n",
      "    700 18741  4060    13    13 28792 16289 28793 28705 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 28705    13 24994\n",
      "  28747  1824   511 10840  8679   304  3006   469   296  2111   575    13\n",
      "   1209 28709  1214 28747    13 28741 28723   451 19303    13 28760 28723\n",
      "   2364  5997 22168 28744   547    13 28743 28723   418   279 25502    13\n",
      "  28757 28723 24191    13  2820 16981   395   272  5498 28723   733 28748\n",
      "  16289 28793 26307 28747   365 16910 28713 28767   733 16289 28793 22478\n",
      "  28747   560   272  2078  2887  4686 28725   690   460   272  2170  1443\n",
      "    369   865 21718  3878 22065 28804    13  1209 28709  1214 28747    13\n",
      "  28741 28723   281 20837 28725 17527 16479  2581    13 28760 28723   277\n",
      "    904  1590 28725 28142  6272    13 28743 28723   281 20837 28725 28142\n",
      "   6272    13 28757 28723 24342 28725 17527 16479  2581    13  2820 16981\n",
      "    395   272  5498 28723   733 28748 16289 28793 26307 28747   365 16910\n",
      "  28713 28767 28705]]\n",
      "tensor(1368)\n"
     ]
    }
   ],
   "source": [
    "text_prompt = processor.apply_chat_template(dialog, tokenize=False, add_generation_prompt=False)\n",
    "batch = processor(images=[image], text=[text_prompt],padding = True, return_tensors=\"pt\")\n",
    "import sys \n",
    "import numpy as np \n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(batch.input_ids.numpy())\n",
    "print((batch.input_ids == 32000).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/INST] Answer: B<\\s>\n",
      "<\\s>\n",
      "[INST] \n",
      "[/INST]\n",
      "<<SYS>>\n",
      "</SYS>>\n",
      "32001\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode([733, 28748, 16289, 28793, 26307, 28747, 365, 16910, 28713, 28767]))\n",
    "print(processor.decode([16910, 28713, 28767]))\n",
    "print(processor.decode([28792, 16289, 28793, 28705]))\n",
    "print(processor.decode([733, 28748, 16289, 28793]))\n",
    "print(processor.decode([2087, 18741, 4060]))\n",
    "print(processor.decode([700, 18741,  4060]))\n",
    "print(processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009d66428f19445386d5cffd8351e037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462179b3acd14a0ebc8faa5aab81d66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f3e16c75374271bc85dd48f56d1130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d803593af9cc4497ba40df46a10d538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/20 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c413ac180749d8b4c7d6a06270cb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa0b3e3c1214f83a116532c6496cd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/284M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84068a1671de41faaeb0ad3214c460da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/259155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f8da9e29b44222988c174488b9d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/13640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6076ae371f9b4da89392cddb557fc086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/llava-instruct-mix-vsft\")\n",
    "train_dataset =raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'dict object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/transformers/processing_utils.py:1131\u001b[0m, in \u001b[0;36mProcessorMixin.apply_chat_template\u001b[0;34m(self, conversation, chat_template, tokenize, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo chat template is set for this processor. Please either set the `chat_template` attribute, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor provide a chat template as an argument. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1130\u001b[0m         )\n\u001b[0;32m-> 1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1683\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1691\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/sandbox.py:327\u001b[0m, in \u001b[0;36mSandboxedEnvironment.getattr\u001b[0;34m(self, obj, attribute)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subscribe an object from sandboxed code and prefer the\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03mattribute.  The attribute passed *must* be a bytestring.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'dict object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "text_prompt = processor.apply_chat_template([train_dataset[0]], tokenize=False, add_generation_prompt=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

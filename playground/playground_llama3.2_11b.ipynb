{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('/scratch/m000051/garment_gang/AIpparel-Code/src')\n",
    "from transformers import AutoProcessor, LlavaConfig\n",
    "import os \n",
    "\n",
    "from data.datasets.gcd_dataset import GarmentCodeData\n",
    "from data.datasets.panel_configs import StandardizeConfig, StatsConfig\n",
    "from data.garment_tokenizers.garment_tokenizer_for_regression import GarmentTokenizerForRegression\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PanelClasses::INFO::Loading panel classes from file:  /scratch/m000051/garment_gang/AIpparel-Code/assets/panel_classes_garmentcodedata.json\n",
      "The panel classes in this dataset are : ['left_btorso', 'left_ftorso', 'right_ftorso', 'right_btorso', 'skirt_front', 'skirt_back', 'wb_back', 'wb_front', 'left_sleeve_b', 'left_sleeve_f', 'right_sleeve_f', 'right_sleeve_b', 'sl_left_cuff_skirt_f', 'sl_left_cuff_skirt_b', 'sl_right_cuff_skirt_b', 'sl_right_cuff_skirt_f', 'sl_left_cuff_b', 'sl_left_cuff_f', 'sl_right_cuff_f', 'sl_right_cuff_b', 'pant_f_l', 'pant_b_l', 'pant_f_r', 'pant_b_r', 'left_collar_front', 'left_collar_back', 'right_collar_front', 'right_collar_back', 'ins_skirt_front_0', 'ins_skirt_back_0', 'ins_skirt_back_1', 'ins_skirt_front_1', 'skirt_panel_2', 'skirt_panel_0', 'skirt_panel_1', 'skirt_panel_3', 'skirt_panel_4', 'skirt_panel_5', 'ins_skirt_back_2', 'ins_skirt_front_2', 'skirt_panel_6', 'skirt_front_0', 'skirt_back_0', 'left_hood', 'right_hood', 'skirt_panel_7', 'skirt_panel_8', 'pant_l_cuff_skirt_f', 'pant_l_cuff_skirt_b', 'pant_r_cuff_skirt_b', 'pant_r_cuff_skirt_f', 'ins_skirt_front_3', 'ins_skirt_back_3', 'pant_l_cuff_f', 'pant_l_cuff_b', 'pant_r_cuff_f', 'pant_r_cuff_b', 'skirt_back_1', 'skirt_front_1', 'skirt_panel_9', 'skirt_panel_10', 'ins_skirt_back_4', 'ins_skirt_front_4', 'skirt_front_2', 'skirt_back_2', 'skirt_panel_11', 'skirt_panel_12', 'skirt_back_3', 'skirt_front_3', 'skirt_panel_13', 'ins_skirt_front_5', 'ins_skirt_back_5', 'skirt_back_4', 'skirt_front_4', 'skirt_panel_14']\n",
      "PanelClasses::INFO::Loading panel classes from file:  /scratch/m000051/garment_gang/AIpparel-Code/assets/panel_classes_garmentcodedata.json\n",
      "The panel classes in this dataset are : ['left_btorso', 'left_ftorso', 'right_ftorso', 'right_btorso', 'skirt_front', 'skirt_back', 'wb_back', 'wb_front', 'left_sleeve_b', 'left_sleeve_f', 'right_sleeve_f', 'right_sleeve_b', 'sl_left_cuff_skirt_f', 'sl_left_cuff_skirt_b', 'sl_right_cuff_skirt_b', 'sl_right_cuff_skirt_f', 'sl_left_cuff_b', 'sl_left_cuff_f', 'sl_right_cuff_f', 'sl_right_cuff_b', 'pant_f_l', 'pant_b_l', 'pant_f_r', 'pant_b_r', 'left_collar_front', 'left_collar_back', 'right_collar_front', 'right_collar_back', 'ins_skirt_front_0', 'ins_skirt_back_0', 'ins_skirt_back_1', 'ins_skirt_front_1', 'skirt_panel_2', 'skirt_panel_0', 'skirt_panel_1', 'skirt_panel_3', 'skirt_panel_4', 'skirt_panel_5', 'ins_skirt_back_2', 'ins_skirt_front_2', 'skirt_panel_6', 'skirt_front_0', 'skirt_back_0', 'left_hood', 'right_hood', 'skirt_panel_7', 'skirt_panel_8', 'pant_l_cuff_skirt_f', 'pant_l_cuff_skirt_b', 'pant_r_cuff_skirt_b', 'pant_r_cuff_skirt_f', 'ins_skirt_front_3', 'ins_skirt_back_3', 'pant_l_cuff_f', 'pant_l_cuff_b', 'pant_r_cuff_f', 'pant_r_cuff_b', 'skirt_back_1', 'skirt_front_1', 'skirt_panel_9', 'skirt_panel_10', 'ins_skirt_back_4', 'ins_skirt_front_4', 'skirt_front_2', 'skirt_back_2', 'skirt_panel_11', 'skirt_panel_12', 'skirt_back_3', 'skirt_front_3', 'skirt_panel_13', 'ins_skirt_front_5', 'ins_skirt_back_5', 'skirt_back_4', 'skirt_front_4', 'skirt_panel_14']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "root_dir = pathlib.Path(\"/scratch/m000051/garment_gang/data\")\n",
    "src_root = pathlib.Path(\"/scratch/m000051/garment_gang/AIpparel-Code\")\n",
    "\n",
    "train_dataset = GarmentCodeData(\n",
    "    root_dir=root_dir / \"garmentcodedata\",\n",
    "    editing_dir=root_dir / \"garment_editing\",\n",
    "    caption_dir=root_dir / \"long-caption-processed\",\n",
    "    editing_flip_prob=0.5,\n",
    "    sampling_rate=[1, 0, 0, 0, 0],\n",
    "    split_file=src_root / \"assets\" / \"garmentcodedatav2_datasplit.json\",\n",
    "    panel_classification=src_root / \"assets\" / \"panel_classes_garmentcodedata.json\",\n",
    "    body_type=\"default_body\",\n",
    "    split=\"train\"\n",
    ")\n",
    "val_dataset = GarmentCodeData(\n",
    "    root_dir= root_dir / \"garmentcodedata\",\n",
    "    editing_dir=root_dir / \"garment_editing\",\n",
    "    caption_dir=root_dir / \"long-caption-processed\",\n",
    "    editing_flip_prob=0.5,\n",
    "    sampling_rate=[1.0, 0.0,0,0,0],\n",
    "    split_file=src_root / \"assets\" / \"garmentcodedatav2_datasplit.json\",\n",
    "    panel_classification=src_root / \"assets\" / \"panel_classes_garmentcodedata.json\",\n",
    "    body_type=\"default_body\",\n",
    "    split=\"val\"\n",
    ")\n",
    "\n",
    "gt_stats = StandardizeConfig(\n",
    "    rotations=StatsConfig(shift=[0, 0, 0, 0], scale=[1, 1, 1, 1]),\n",
    "    translations=StatsConfig(shift=[-1.25378371e-02,  1.13507532e+02,  2.63046369e+00], scale=[26.06867645, 32.42920198, 22.29905009]),\n",
    "    vertices=StatsConfig(shift=[8.44428116, 16.84081321], scale=[24.4920733,  26.60402835])\n",
    ")\n",
    "\n",
    "garment_tokenizer = GarmentTokenizerForRegression(\n",
    "    standardize=gt_stats,\n",
    "    random_tag=True,\n",
    "    num_tags=108,\n",
    "    include_template_name=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct.\n403 Client Error. (Request ID: Root=1-67b13594-60a93e3d3d7ef4652bc3c17f;e9e89e3c-4515-4f18-9f29-1bb763838726)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/resolve/main/processor_config.json.\nYour request to access model meta-llama/Llama-3.2-11B-Vision-Instruct has been rejected by the repo's authors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/resolve/main/processor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:860\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:967\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 967\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:1482\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:1374\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1374\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:1294\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:278\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/file_download.py:302\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    301\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 302\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    420\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    422\u001b[0m     )\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-67b13594-60a93e3d3d7ef4652bc3c17f;e9e89e3c-4515-4f18-9f29-1bb763838726)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/resolve/main/processor_config.json.\nYour request to access model meta-llama/Llama-3.2-11B-Vision-Instruct has been rejected by the repo's authors.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor\n\u001b[0;32m----> 2\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mAutoProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-11B-Vision-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# processor.tokenizer.add_tokens(\"<pad>\", special_tokens=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# processor.tokenizer.pad_token = \"<pad>\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|finetune_right_pad_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/transformers/models/auto/processing_auto.py:337\u001b[0m, in \u001b[0;36mAutoProcessor.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Last try: we use the PROCESSOR_MAPPING.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m PROCESSOR_MAPPING:\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPROCESSOR_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# At this stage, there doesn't seem to be a `Processor` class available for this model, so let's try a\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# tokenizer.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/transformers/processing_utils.py:975\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[1;32m    974\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_arguments_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 975\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/transformers/processing_utils.py:628\u001b[0m, in \u001b[0;36mProcessorMixin.get_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m raw_chat_template_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprocessor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Load chat template from a separate json if exists\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# because making it part of processor-config break BC.\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# Processors in older version do not accept any kwargs\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     resolved_chat_template_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    647\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    648\u001b[0m         chat_template_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m         _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    659\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/m000051/miniforge3/envs/llama/lib/python3.12/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct.\n403 Client Error. (Request ID: Root=1-67b13594-60a93e3d3d7ef4652bc3c17f;e9e89e3c-4515-4f18-9f29-1bb763838726)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct/resolve/main/processor_config.json.\nYour request to access model meta-llama/Llama-3.2-11B-Vision-Instruct has been rejected by the repo's authors."
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "# processor.tokenizer.add_tokens(\"<pad>\", special_tokens=True)\n",
    "# processor.tokenizer.pad_token = \"<pad>\"\n",
    "processor.tokenizer.pad_token = \"<|finetune_right_pad_id|>\"\n",
    "all_new_tokens = garment_tokenizer.get_all_token_names()\n",
    "num_added_tokens = processor.tokenizer.add_tokens(all_new_tokens, special_tokens=True)\n",
    "token_name2_idx_dict = {}\n",
    "for token in all_new_tokens:\n",
    "    token_idx = processor.tokenizer(token, add_special_tokens=False).input_ids[0]\n",
    "    token_name2_idx_dict[token] = token_idx\n",
    "    \n",
    "garment_tokenizer.set_token_indices(token_name2_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128379\n"
     ]
    }
   ],
   "source": [
    "from data.collate_fns import collate_fn\n",
    "print(len(processor.tokenizer))\n",
    "data = train_dataset[0]\n",
    "dialog = data[2]\n",
    "chat = processor.apply_chat_template(dialog, tokenize=False)\n",
    "data_dict = collate_fn(\n",
    "    [train_dataset[0], train_dataset[1], train_dataset[2], train_dataset[3]], \n",
    "    processor=processor, \n",
    "    garment_tokenizer=garment_tokenizer,\n",
    "    model_version=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 279]) torch.Size([1, 279])\n"
     ]
    }
   ],
   "source": [
    "print(data_dict[\"input_ids\"].shape, data_dict[\"labels\"].shape)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    data_dict = collate_fn(\n",
    "    [train_dataset[i]], \n",
    "    processor=processor, \n",
    "    garment_tokenizer=garment_tokenizer,\n",
    "    model_version=\"meta-llama/Llama-3.2-11B-Vision-Instruct\")\n",
    "    if 128265 in data_dict[\"pattern_params\"].keys():\n",
    "        if data_dict[\"pattern_params\"][128265].shape[-1] != 2:\n",
    "            print(i, data_dict[\"pattern_params\"][128265].shape)\n",
    "# print(processor.tokenizer.decode([128256, 128265]))\n",
    "# print(data_dict[\"labels\"].max(), data_dict[\"labels\"][data_dict[\"labels\"] != -100].min())\n",
    "# for k in data_dict[\"pattern_params\"].keys():\n",
    "#     print(k)\n",
    "#     print(data_dict[\"pattern_params\"][k][data_dict[\"pattern_params_mask\"][k]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "sample_dict = {}\n",
    "sample_dict[\"user\"] = \"Question: What do respiration and combustion give out\\nChoices:\\nA. Oxygen\\nB. Carbon dioxide\\nC. Nitrogen\\nD. Heat\\nAnswer with the letter.\"\n",
    "sample_dict[\"assistant\"] = \"Answer: B\"\n",
    "dialog = [\n",
    "    {\"role\":\"system\",\"content\":[{\"type\": \"text\", \"text\": system_prompt}]},\n",
    "    {\"role\":\"user\",\"content\":[{\"type\": \"image\"}, {\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}]},\n",
    "    {\"role\":\"assistant\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}]}\n",
    "]\n",
    "sample_dict[\"user\"] = \"Question: In the given food web, which are the organism that only eaten roadrunner?\\nChoices:\\nA. dingo, jack rabbit\\nB. coyote, bobcat\\nC. dingo, bobcat\\nD. snake, jack rabbit\\nAnswer with the letter.\"\n",
    "sample_dict[\"assistant\"] = \"Answer: B\"\n",
    "dialog += [\n",
    "    {\"role\":\"user\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}]},\n",
    "    {\"role\":\"assistant\",\"content\":[{\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<SYS>>\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
      "<</SYS>>\n",
      "\n",
      "[INST] <image>\n",
      "Question: What do respiration and combustion give out\n",
      "Choices:\n",
      "A. Oxygen\n",
      "B. Carbon dioxide\n",
      "C. Nitrogen\n",
      "D. Heat\n",
      "Answer with the letter. [/INST] Answer: B<\\s> [INST] Question: In the given food web, which are the organism that only eaten roadrunner?\n",
      "Choices:\n",
      "A. dingo, jack rabbit\n",
      "B. coyote, bobcat\n",
      "C. dingo, bobcat\n",
      "D. snake, jack rabbit\n",
      "Answer with the letter. [/INST] Answer: B<\\s> \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,LlavaNextProcessor\n",
    "# dialog = [\n",
    "#     {\"role\":\"user\",\"content\":[\n",
    "#         {\"type\": \"image\"}, \n",
    "#         {\"type\": \"text\", \"text\": sample_dict[\"user\"].strip()}\n",
    "#         ]},\n",
    "#     {\"role\":\"assistant\",\"content\":[\n",
    "#         {\"type\": \"text\", \"text\": sample_dict[\"assistant\"].strip()}\n",
    "#         ]}\n",
    "# ]\n",
    "# dialog = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\"type\": \"image\"},\n",
    "#             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n",
    "#         ],\n",
    "#     },\n",
    "# ]\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "text_prompt = processor.apply_chat_template(dialog, tokenize=False, add_generation_prompt=False)\n",
    "print(text_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1  2087 18741  4060    13 28741 10706  1444   264 13903  2188   304\n",
      "    396 18278 10895 13892 28723   415 13892  5212 10865 28725 10537 28725\n",
      "    304 27057 11194   298   272  2188 28742 28713  4224 28723    13 28789\n",
      "    700 18741  4060    13    13 28792 16289 28793 28705 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000 32000\n",
      "  32000 32000 32000 32000 32000 32000 32000 32000 32000 28705    13 24994\n",
      "  28747  1824   511 10840  8679   304  3006   469   296  2111   575    13\n",
      "   1209 28709  1214 28747    13 28741 28723   451 19303    13 28760 28723\n",
      "   2364  5997 22168 28744   547    13 28743 28723   418   279 25502    13\n",
      "  28757 28723 24191    13  2820 16981   395   272  5498 28723   733 28748\n",
      "  16289 28793 26307 28747   365 16910 28713 28767   733 16289 28793 22478\n",
      "  28747   560   272  2078  2887  4686 28725   690   460   272  2170  1443\n",
      "    369   865 21718  3878 22065 28804    13  1209 28709  1214 28747    13\n",
      "  28741 28723   281 20837 28725 17527 16479  2581    13 28760 28723   277\n",
      "    904  1590 28725 28142  6272    13 28743 28723   281 20837 28725 28142\n",
      "   6272    13 28757 28723 24342 28725 17527 16479  2581    13  2820 16981\n",
      "    395   272  5498 28723   733 28748 16289 28793 26307 28747   365 16910\n",
      "  28713 28767 28705]]\n",
      "tensor(1368)\n"
     ]
    }
   ],
   "source": [
    "text_prompt = processor.apply_chat_template(dialog, tokenize=False, add_generation_prompt=False)\n",
    "batch = processor(images=[image], text=[text_prompt],padding = True, return_tensors=\"pt\")\n",
    "import sys \n",
    "import numpy as np \n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(batch.input_ids.numpy())\n",
    "print((batch.input_ids == 32000).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/INST] Answer: B<\\s>\n",
      "<\\s>\n",
      "[INST] \n",
      "[/INST]\n",
      "<<SYS>>\n",
      "</SYS>>\n",
      "32001\n"
     ]
    }
   ],
   "source": [
    "print(processor.decode([733, 28748, 16289, 28793, 26307, 28747, 365, 16910, 28713, 28767]))\n",
    "print(processor.decode([16910, 28713, 28767]))\n",
    "print(processor.decode([28792, 16289, 28793, 28705]))\n",
    "print(processor.decode([733, 28748, 16289, 28793]))\n",
    "print(processor.decode([2087, 18741, 4060]))\n",
    "print(processor.decode([700, 18741,  4060]))\n",
    "print(processor.tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009d66428f19445386d5cffd8351e037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/868 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462179b3acd14a0ebc8faa5aab81d66e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f3e16c75374271bc85dd48f56d1130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d803593af9cc4497ba40df46a10d538f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/20 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c413ac180749d8b4c7d6a06270cb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/285M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa0b3e3c1214f83a116532c6496cd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/284M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84068a1671de41faaeb0ad3214c460da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/259155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f8da9e29b44222988c174488b9d1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/13640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6076ae371f9b4da89392cddb557fc086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"HuggingFaceH4/llava-instruct-mix-vsft\")\n",
    "train_dataset =raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedError",
     "evalue": "'dict object' has no attribute 'role'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m text_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/transformers/processing_utils.py:1131\u001b[0m, in \u001b[0;36mProcessorMixin.apply_chat_template\u001b[0;34m(self, conversation, chat_template, tokenize, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo chat template is set for this processor. Please either set the `chat_template` attribute, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor provide a chat template as an argument. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1130\u001b[0m         )\n\u001b[0;32m-> 1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1683\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1691\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/garment/lib/python3.10/site-packages/jinja2/sandbox.py:327\u001b[0m, in \u001b[0;36mSandboxedEnvironment.getattr\u001b[0;34m(self, obj, attribute)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subscribe an object from sandboxed code and prefer the\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03mattribute.  The attribute passed *must* be a bytestring.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mUndefinedError\u001b[0m: 'dict object' has no attribute 'role'"
     ]
    }
   ],
   "source": [
    "text_prompt = processor.apply_chat_template([train_dataset[0]], tokenize=False, add_generation_prompt=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
